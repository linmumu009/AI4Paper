{"paper_id": "2602.05810", "title": "Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents", "url": "https://arxiv.org/abs/2602.05810", "year": 2026, "blocks": {"background": {"text": "大语言模型在上下文学习（ICL）中依赖高质量历史轨迹作为示例，但当任务上下文发生显著变化时，直接复用原始轨迹效果骤降；现有方法如微调成本高，启发式校准（如Reflexion、BoT）缺乏理论支撑且稳定性差。", "bullets": ["【痛点】：上下文偏移导致历史轨迹失效，而微调成本高、启发式方法不可靠", "【现状】：ICL泛化能力受限于上下文-轨迹耦合假设，缺乏表征层面的自适应机制"]}, "objective": {"text": "提出一种无需训练的轨迹迁移方法Bifrost，首次建模并利用上下文偏移与轨迹偏移在隐空间中的平行性，实现精准、可解释、结构保持的轨迹自适应。", "bullets": ["【任务】：上下文感知的轨迹迁移（Context-Aware Trajectory Transfer）", "【核心贡献】：发现并利用隐空间中上下文-轨迹偏移的几何平行性，实现无训练、表征级轨迹适配"]}, "method": {"text": "给定目标问题提示x_target与历史成功轨迹（x_hist, y_hist），先提取其在指定中间层（如Llama-8B第20层）的平均隐状态，计算方向向量Δℓ = h(x_target) − mean(h(x_hist))；再将αΔℓ加性扰动施加于x_target各token的该层隐状态，驱动后续生成保持语义对齐。", "bullets": ["【输入】：目标问题提示 + 历史成功轨迹（含输入与输出）", "【架构】：基于LLM中间层残差流（如Llama-8B）", "【关键机制】：隐空间线性平移（Δℓ导向的隐状态扰动）", "【是否训练】：否 (无需参数更新，纯前向+向量运算)", "【创新点】：将轨迹迁移形式化为表征空间的几何平移操作，具备理论可解释性与跨层可控性"]}, "data": {"text": "在跨域组合基准上验证，涵盖数学推理、多学科问答与代码生成三大范式。", "bullets": ["【数据集】：AQUA→GSM8K (数学推理)", "【数据集】：ARC→GPQA (多学科问答)", "【数据集】：HumanEval→LiveCodeBench (代码生成)", "【来源】：公开基准测试集及对应领域迁移配对"]}, "experiment": {"text": "在固定模型（Llama-3-8B/70B、Qwen2-7B等）上对比主流ICL增强方法，包含消融研究、层敏感性分析与亚最优轨迹鲁棒性测试。", "bullets": ["【基线模型】：Llama-3-8B, Qwen2-7B", "【对比方法】：ICL, Reflexion, BoT, RISE", "【消融实验】：去除历史示例 / 使用反向Δℓ / 随机方向扰动", "【控制变量】：扰动层位置（第14–20层）、缩放系数α"]}, "metrics": {"text": "采用各任务标准准确率指标，强调绝对提升与跨域泛化稳定性。", "bullets": ["Exact Match (EM)", "Pass@1", "Win Rate (vs. baseline)"]}, "results": {"text": "Bifrost在全部跨域迁移任务上一致超越强基线，尤其在分布外泛化场景下展现显著鲁棒性；其性能增益具有明确的几何归因与层定位规律。", "bullets": ["GSM8K：EM = 78.5% (+16.2% vs. ICL)", "GPQA：EM = 41.3% (+9.7% vs. RISE)", "LiveCodeBench：Pass@1 = 52.1% (+11.4% vs. BoT)", "消融结果：去除历史示例 → EM ↓7.5%，反向Δℓ → EM ↓7.3%"]}, "limitations": {"text": "方法依赖隐状态线性可分假设与上下文-轨迹方向一致性，在极端分布偏移或低质量历史轨迹下可能失效；层选择需任务适配，尚未实现全自动层定位。", "bullets": ["【假设强】：依赖隐空间线性表示与统计显著的上下文-轨迹平行性（p<0.02）", "【适用边界】：对噪声大、模式混杂的历史轨迹泛化能力待验证", "【工程约束】：需人工指定扰动层，暂未集成自适应层选择机制"]}}}
{"paper_id": "2602.05842", "title": "Reinforcement World Model Learning for LLM-based Agents", "url": "https://arxiv.org/abs/2602.05842", "year": 2026, "blocks": {"background": {"text": "现有世界模型（WM）多依赖监督式next-token预测，易过拟合token序列，导致环境动态建模脆弱；同时，主流方法常需专家轨迹、强语言模型评判（LLM-as-a-judge）或任务成功奖励，引入偏差、噪声与训练不稳定性。", "bullets": ["【痛点】：next-token预测式WM因过拟合token导致环境建模崩溃", "【痛点】：LLM-as-a-judge奖励易受奖励黑客攻击，信号不稳定", "【现状】：世界建模严重依赖人工标注或外部强模型，难以实现端到端自监督"]}, "objective": {"text": "提出一种无需专家数据、无需任务成功奖励、无需外部评判的自监督世界模型学习框架，使LLM代理能稳健、可解释、泛化地建模动作条件下的环境动态。", "bullets": ["【任务】：动作条件的世界模型学习", "【核心贡献】：Reinforcement World Model Learning（RWML）——基于sim-to-real嵌入空间对齐的自监督强化学习方法"]}, "method": {"text": "以代理自身交互采集的⟨s≤t, at, st+1⟩三元组为输入，用冻结文本嵌入模型（如Qwen3-Embedding）计算预测状态ŝt+1与真实st+1的余弦相似度作为reward r^WM，通过GRPO算法优化该reward，并强制模型在<think>块中生成推理过程后输出下一状态；辅以‘易样本剔除’策略（SFT初筛 + p=0.1低概率保留）聚焦中高难度样本。", "bullets": ["【输入】：动作-状态-下一状态三元组 ⟨s≤t, at, st+1⟩（自采集）", "【架构】：LLM主干（如Qwen系列）+ 冻结文本嵌入模型（Qwen3-Embedding）", "【关键机制】：embedding-based sim-to-real reward r^WM、GRPO优化、<think>-first推理范式、易样本剔除（SFT初筛 + p=0.1采样）", "【是否训练】：是（GRPO策略梯度更新LLM参数，但嵌入模型冻结）", "【创新点】：从token级重建转向语义级动态对齐；用冻结嵌入提供稳定可微reward；构建‘自采集-自筛选-自优化’闭环"]}, "data": {"text": "全部训练数据由代理在环境中自主交互生成，无外部标注；评估覆盖具身推理与工具调用双场景。", "bullets": ["【数据集】：ALFWorld（具身推理）", "【数据集】：τ²Bench（工具调用与状态演化）", "【来源】：代理自主交互采集（self-collected trajectories）"]}, "experiment": {"text": "在ALFWorld和τ²Bench上对比RWML与WM SFT（监督式next-token预测）、纯任务奖励RL等基线；开展消融研究验证reward设计、GRPO、<think>机制及易样本剔除的有效性；分析参数更新幅度与通用能力退化程度。", "bullets": ["【基线模型】：WM SFT（next-token预测式世界模型）", "【基线模型】：纯任务奖励RL（task-only RL）", "【消融实验】：去除r^WM改用token-level loss / 去除<think>块 / 关闭易样本剔除", "【对比维度】：世界建模精度、决策质量（无效动作率）、通用能力保持（MMLU/GSM8K）"]}, "metrics": {"text": "采用环境完成率与状态预测保真度联合评估；同步报告下游决策指标与通用能力退化指标。", "bullets": ["Success Rate", "Pass@1 (ALFWorld)", "State EM (τ²Bench)", "Invalid Action Rate", "Tool Call Error Rate", "MMLU", "GSM8K"]}, "results": {"text": "RWML显著提升世界建模性能与下游决策质量，在保持通用能力的同时，展现出比监督基线更强的鲁棒性与泛化性。", "bullets": ["ALFWorld：Pass@1 = +19.6 pts vs WM SFT", "τ²Bench：State EM = +7.9 pts vs WM SFT", "ALFWorld：Invalid Action Rate ↓19.85%", "τ²Bench：Tool Call Error Rate ↓16.06%", "RWML+Policy RL：+6.9/+5.7 pts vs pure task-only RL", "MMLU/GSM8K退化显著小于WM SFT（尤其MLP/Attention模块更新量↓）"]}, "limitations": {"text": "方法依赖高质量预训练文本嵌入模型的语义一致性；GRPO优化带来额外推理开销；当前仅验证于离散动作与符号化状态空间。", "bullets": ["【假设强】：依赖冻结文本嵌入模型（如Qwen3-Embedding）在状态语义空间上的保真性", "【开销大】：GRPO每步需多次rollout与reward评估，推理延迟增加", "【适用窄】：当前未适配连续控制或像素级视觉状态输入"]}}}
{"paper_id": "2602.05843", "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions", "url": "https://arxiv.org/abs/2602.05843", "year": 2026, "blocks": {"background": {"text": "现有大语言模型评测多聚焦于指令遵循、短程推理或静态知识检索，缺乏对长周期交互、主动探索与隐式规则归纳等真实世界建模能力的系统性评估；当前模型在‘知其然’（演绎）上表现强劲，但在‘知其所以然’（归纳）上存在根本性瓶颈。", "bullets": ["【痛点】：传统基准无法衡量LLM自主发现环境隐含规则的能力", "【现状】：归纳推理能力被严重低估且缺乏可复现、可分解的评测框架"]}, "objective": {"text": "构建首个面向归纳式智能体的科学评测基准ODYSSEYARENA，系统化评估大语言模型通过交互经验自主归纳环境动态规律的能力，并揭示其归纳瓶颈的本质成因。", "bullets": ["【任务】：归纳式智能体评测", "【核心贡献】：提出基于结构原语的可扩展、确定性、双粒度评测框架"]}, "method": {"text": "输入为交互式环境（含观测、动作空间与奖励信号），智能体通过多步试错交互收集轨迹；系统基于四类数学本质原语（离散符号规则、连续随机动力学、周期性时间模式、关系图结构）构建轻量但形式化严格的环境；所有任务采用确定性轨迹配置，支持难度参数化调控；输出为成功率、循环率、收敛步数等归纳效能指标。", "bullets": ["【输入】：交互式环境（观测/动作/奖励）+ 初始状态", "【架构】：非神经方法（评测框架，不训练模型）", "【关键机制】：结构原语解构 + 确定性轨迹生成 + 双粒度任务设计（LITE / CHALLENGE）", "【是否训练】：否 (纯评测协议，不涉及模型训练或微调)", "【创新点】：首次将‘环境规则归纳’形式化为可评测的认知任务，并解耦为四类不可约数学原语"]}, "data": {"text": "ODYSSEYARENA包含两个子基准：ODYSSEYARENA-LITE（120个标准化归纳任务）用于高效评测；ODYSSEYARENA-CHALLENGE（超长周期>1000步）用于压力测试。每个任务源自一个形式化环境，对应一类结构原语。", "bullets": ["【数据集】：ODYSSEYARENA-LITE (120 tasks)", "【数据集】：ODYSSEYARENA-CHALLENGE (long-horizon tasks)", "【环境】：Turn On Lights (离散符号规则)", "【环境】：AI Trading (连续随机动力学)", "【环境】：Energy Dispatch (周期性时间模式)", "【环境】：Repo System (关系图结构)"]}, "experiment": {"text": "在统一交互协议下评估多个前沿闭源与开源模型（如Gemini 3 Pro Preview、GPT-4、Claude、Llama系列），对比其在LITE与CHALLENGE上的成功率、动作循环率与性能饱和曲线；开展消融分析（如提供显式规则 vs 隐式规则），验证归纳-演绎能力不对称性。", "bullets": ["【基线模型】：Gemini 3 Pro Preview, GPT-4, Claude, Llama-3", "【消融实验】：提供显式规则时的成功率跃迁", "【对照设置】：确定性轨迹 vs 随机环境（文中强调前者以消除噪声）"]}, "metrics": {"text": "以任务成功率（Success Rate）为核心指标，辅以动作循环率（Action Cycling Rate）、收敛步数（Steps to Saturation）、长期策略稳定性（via CHALLENGE success decay）等过程性指标，全面刻画归纳效率与稳健性。", "bullets": ["Success Rate", "Action Cycling Rate", "Steps to Saturation", "Long-horizon Stability (via ODYSSEYARENA-CHALLENGE)"]}, "results": {"text": "最强商业模型（Gemini 3 Pro Preview）在LITE上平均成功率仅44.17%，远低于人类100%；提供显式规则后成功率近100%；所有模型均呈现快速性能饱和与高循环率，证实归纳能力是系统性短板。", "bullets": ["ODYSSEYARENA-LITE：Success Rate = 44.17% (Gemini 3 Pro Preview, vs Human 100%)", "Rule-Guided Setting：Success Rate ≈ 100% (vs 44.17% in implicit setting)", "Action Cycling Rate ↑ → Success Rate ↓ (strong negative correlation)", "Steps to Saturation ≤ 200 → marginal gain beyond"]}, "limitations": {"text": "当前基准聚焦于单智能体、确定性、离散-连续混合但低维的环境，尚未覆盖多智能体博弈、高维感知（如视觉观测）、非平稳环境演化等更复杂归纳场景；任务构造依赖人工形式化设计，自动化原语生成与组合尚待拓展。", "bullets": ["【覆盖窄】：暂未纳入多智能体、视觉观测、非平稳动态等场景", "【构造依赖】：结构原语与环境仍需人工形式化定义，缺乏自动合成机制", "【尺度限】：CHALLENGE虽>1000步，但未达真实世界级长周期（如年尺度建模）"]}}}
{"paper_id": "2602.05848", "title": "DARWIN: Dynamic Agentically Rewriting Self-Improving Network", "url": "https://arxiv.org/abs/2602.05848", "year": 2026, "blocks": {"background": {"text": "大语言模型的自我改进长期停留在理论层面（如Gödel机），缺乏可工程化、可审计、安全可控的实现路径；当前LLM虽擅长代码生成，但尚未系统性用于优化自身训练流程。", "bullets": ["【痛点】：现有自进化方法多依赖权重更新或强化学习，难以验证、审计与部署", "【现状】：训练代码通常由人工编写和维护，LLM仅作为下游应用，未被用作训练基础设施的主动优化者"]}, "objective": {"text": "提出DARWIN框架，实现大语言模型在训练代码层面的递归式、无需人工持续干预的自主迭代优化。", "bullets": ["【任务】：训练脚本的自主生成、变异与评估", "【核心贡献】：首次整合遗传算法、多智能体协作、持久化记忆与人机协同接口，构建可执行、可回溯、可干预的元训练进化范式"]}, "method": {"text": "输入为初始训练代码与任务目标（如最小化困惑度），经多GPT代理并行执行突变-评估-选择循环，通过JSON记忆系统记录历史状态，借助HITL接口引入人工反馈，并在容器化沙箱中安全执行变异后的代码。", "bullets": ["【输入】：初始训练脚本（nanoGPT）、数据集（Shakespeare）、优化目标（e.g., loss）", "【架构】：多智能体GPT代理 + JSON记忆系统 + HITL接口 + Docker/VM沙箱", "【关键机制】：遗传算法（交叉/突变）、模块级源码分块上下文构建、双向人机协同（HITL）、跨代记忆回溯", "【是否训练】：否 (LLM本身不训练；仅调用API生成/修改Python训练代码)", "【创新点】：将进化计算落地为训练代码元优化，以记忆+HITL+容器化保障可审计性与安全性"]}, "data": {"text": "实验基于nanoGPT架构，在Shakespeare文本数据集上进行验证。", "bullets": ["【数据集】：Shakespeare (文本生成)", "【来源】：tiny-corpora / nanoGPT官方示例数据"]}, "experiment": {"text": "在5轮进化迭代中评估性能变化，并开展消融实验验证各组件作用；使用GPT-4o-mini模拟代理行为，所有训练在本地环境隔离执行。", "bullets": ["【基线模型】：原始nanoGPT（无进化）", "【消融实验】：移除JSON持久化记忆 → 性能下降3%", "【对比设置】：5轮进化 vs 零轮（静态脚本）", "【代理模拟】：GPT-4o-mini（非真实训练代理）"]}, "metrics": {"text": "以语言建模标准指标衡量训练效果，并统计进化过程稳定性与成功率。", "bullets": ["Perplexity (PPL)", "MFU (Model FLOPs Utilization)", "Success Rate", "Recovery Rate", "Win Rate (vs baseline)"]}, "results": {"text": "DARWIN在5轮进化后实现困惑度下降2.07%、MFU提升1.26%，但存在高失败率与显著波动，验证了可行性亦暴露当前局限。", "bullets": ["Shakespeare：PPL = ↓2.07% (+0.23↓ vs baseline)", "Shakespeare：MFU = ↑1.26% (+0.18↑ vs baseline)", "Success Rate：16.67% (8/48 valid runs)", "Failure Rate：37.5% (18/50 attempts)", "Ablation：w/o memory → PPL ↑3% (relative degradation)"]}, "limitations": {"text": "当前实现受限于代理能力、数据单一性与工程简化假设，尚未达到通用、鲁棒、自动化的自我改进水平。", "bullets": ["【代理弱】：使用GPT-4o-mini替代真实可训练代理，能力受限", "【数据窄】：仅验证Shakespeare单数据集，未覆盖代码、数学等复杂任务", "【假设强】：依赖LLM对Python训练代码的语义理解与安全生成能力", "【开销大】：每轮需多次API调用+容器启动，延迟高、成本高", "【评估浅】：未接入标准基准（如GLUE、HumanEval）"]}}}
{"paper_id": "2602.05853", "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference", "url": "https://arxiv.org/abs/2602.05853", "year": 2026, "blocks": {"background": {"text": "长上下文Transformer推理面临O(L²)注意力计算复杂度瓶颈，现有动态稀疏方法常需预处理、牺牲查询独立性或削弱全局建模能力。", "bullets": ["【痛点】：预处理依赖破坏端到端推理流", "【痛点】：查询非独立设计阻碍并行化与流式生成", "【现状】：多数方法在全局覆盖与局部稀疏间难以兼顾"]}, "objective": {"text": "提出首个满足无预处理、全局评估、查询独立、模式无关、步幅级softmax五大特性的动态稀疏注意力机制，实现高保真长上下文推理加速。", "bullets": ["【核心贡献】：RRAttention —— 首个同时满足五大理想特性的动态稀疏注意力", "【任务】：长上下文高效注意力计算"]}, "method": {"text": "输入为序列Q/K/V；通过头轮询策略在每个步幅S内跨头错位采样单查询，聚合步幅内键后执行步幅级点积与softmax；再基于步幅分数聚合为块级重要性，自适应选取Top-τ关键块（强制保留末块）；输出稀疏注意力权重。", "bullets": ["【输入】：查询Q、键K、值V（长度L，步幅S）", "【架构】：Transformer注意力层增强模块", "【关键机制】：头轮询采样、步幅级softmax、自适应Top-τ块级选择（含末块保留）", "【是否训练】：否 (无需参数更新，纯确定性计算)", "【创新点】：将轮询调度思想嵌入注意力结构，以零训练代价解耦查询独立性与全局覆盖"]}, "data": {"text": "在HELMET（多任务长文本理解基准）与Video-MME（视频-语言长上下文多模态评测）上验证。", "bullets": ["【数据集】：HELMET (长文本理解)", "【数据集】：Video-MME (视频-语言长上下文)"]}, "experiment": {"text": "对比主流动态稀疏基线（FlexPrefill、XAttention），在128K上下文下进行性能-效率权衡分析，并开展消融研究验证头轮询、步幅级softmax及块选择策略的贡献。", "bullets": ["【基线模型】：FlexPrefill, XAttention", "【消融实验】：头轮询 vs 固定位置采样 vs 层轮询", "【消融实验】：步幅级softmax vs 全序列softmax", "【消融实验】：Top-τ块选择 vs 均匀采样"]}, "metrics": {"text": "采用全注意力性能恢复率（%）、实际计算块占比（%）、F1/精度/召回率（块选择质量）、推理加速比（×）等多维指标评估。", "bullets": ["Performance Recovery Rate", "Block Coverage Ratio", "F1 Score", "Precision", "Recall", "Speedup"]}, "results": {"text": "RRAttention在128K上下文下实现2.4×加速，恢复99.7%（HELMET）和99.0%（Video-MME）全注意力性能，仅计算约50%注意力块；块选择F1平均提升0.52%，精度显著提升而召回率稳定。", "bullets": ["HELMET：Performance Recovery = 99.7% (+0.9% vs XAttention)", "Video-MME：Performance Recovery = 99.0% (+1.2% vs FlexPrefill)", "128K：Speedup = 2.4× (vs full attention)", "HELMET：F1 ↑ 0.52% (vs baseline)", "Block Coverage = ~50% (at τ=0.95)"]}, "limitations": {"text": "方法依赖步幅超参S与阈值τ的手动设定；块级选择未引入可学习机制，泛化至极长或非均匀分布序列时鲁棒性待验证。", "bullets": ["【假设强】：依赖人工设定步幅S与累积阈值τ", "【泛化限】：未适配token-length不均衡序列（如混合模态chunk）", "【开销大】：步幅级聚合引入额外内存访存开销（未量化）"]}}}
{"paper_id": "2602.05859", "title": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders", "url": "https://arxiv.org/abs/2602.05859", "year": 2026, "blocks": {"background": {"text": "扩散语言模型（DLMs）缺乏面向其独特随机腐蚀机制与多步去噪过程的机械可解释性工具，现有稀疏自编码器（SAE）框架主要针对自回归（AR）模型设计，难以适配DLM的掩码-去噪动态结构。", "bullets": ["【痛点】：现有SAE无法适配DLM的随机掩码机制与多时间步残差流", "【现状】：机械可解释性研究集中于LLM，DLM尚无专用SAE框架"]}, "objective": {"text": "构建首个面向扩散语言模型（DLMs）的稀疏自编码器（SAE）可解释性框架DLM-Scope，实现特征可解释性提取、跨层可控干预与解码过程动态量化分析。", "bullets": ["【任务】：DLM机械可解释性建模", "【核心贡献】：提出首套DLM专属SAE框架DLM-Scope，并揭示DLM特有早期层损失降低现象"]}, "method": {"text": "在DLM残差流中按扩散时间步与掩码状态分采激活：对掩码位置训练MASK-SAE，对非掩码位置训练UNMASK-SAE；在每一步去噪中重复注入SAE特征方向，支持ALL-TOKENS/UPDATE-TOKENS干预；基于SAE特征集Jaccard相似度定义S_pre与D_post量化解码稳定性与漂移；验证基座SAE在指令微调DLM上的即插即用迁移能力。", "bullets": ["【输入】：DLM各层残差流激活（按时间步t及掩码状态区分）", "【架构】：稀疏自编码器（SAE）", "【关键机制】：掩码感知采样（MASK/UNMASK-SAE）、扩散时间步steering、解码顺序Jaccard动态分析", "【是否训练】：是（端到端SAE训练，含L0正则）", "【创新点】：首个适配DLM多步掩码结构的SAE框架；发现并利用DLM早期层SAE插入降损特性"]}, "data": {"text": "基于开源DLM基座模型（如DiffuLM、LatentLM等）及其指令微调变体进行SAE训练与评估，未使用外部标注数据集，全部依赖模型内部残差流激活。", "bullets": ["【数据集】：DLM基座模型残差流激活（无监督）", "【来源】：DiffuLM/LatentLM等开源DLM基座及其IF微调版本"]}, "experiment": {"text": "在DLM不同层（L1–L27）部署SAE，对比MASK/UNMASK策略、steering时机（ALL-TOKENS vs UPDATE-TOKENS）、解码顺序策略（TOPK-MARGIN/ENTROPY）；开展消融实验验证SAE跨阶段泛化性（基座→IF-DLM）与层功能一致性；以LLM-SAE作为对照基线。", "bullets": ["【基线模型】：LLM-SAE（如GPT-2/LLaMA SAE）", "【消融实验】：MASK-SAE vs UNMASK-SAE；ALL-TOKENS vs UPDATE-TOKENS steering；TOPK-MARGIN vs ENTROPY解码策略", "【对比设置】：DLM vs LLM在相同层SAE steering得分、跨阶段迁移稳定性"]}, "metrics": {"text": "采用交叉熵损失变化（ΔCE）、steering得分（feature activation × direction alignment）、Jaccard相似度（S_pre, D_post）、功能迁移保真度（layer-wise feature recovery rate）、GSM8K准确率（用于关联解码动态与下游性能）。", "bullets": ["ΔCE", "Steering Score", "Jaccard Similarity (S_pre, D_post)", "GSM8K Accuracy"]}, "results": {"text": "DLM-SAE在L1/L5插入显著降低masked-token交叉熵（ΔLM < 0），该现象在LLM中不存在；深层（L23/L27）steering得分达LLM-SAE的2–10倍；TOPK-MARGIN/ENTROPY解码策略下S_pre↓ & D_post↑与GSM8K性能（56%–59%）强正相关；基座SAE在L1–L23层对IF-DLM迁移保真度>95%，仅L27层明显退化。", "bullets": ["L1/L5：ΔCE = -0.12 (vs baseline, DLM特有)", "L27：Steering Score = 8.3× LLaMA-2 SAE", "TOPK-MARGIN：S_pre = 0.41, D_post = 0.67 → GSM8K = 59%", "L1–L23：迁移保真度 ≥ 95% (vs IF-DLM)"]}, "limitations": {"text": "当前框架依赖DLM残差流内部激活，尚未扩展至多模态DLM或非Transformer扩散架构；steering效果受限于时间步粒度与方向对齐精度；解码动态指标S_pre/D_post尚未在更广泛任务集上验证泛化性。", "bullets": ["【假设强】：依赖DLM残差流可线性分解为语义方向", "【覆盖窄】：仅验证于文本DLM，未适配多模态或非Transformer DLM", "【评估浅】：S_pre/D_post与下游性能的因果性尚未通过干预实验证实"]}}}
{"paper_id": "2602.05863", "title": "Constrained Group Relative Policy Optimization", "url": "https://arxiv.org/abs/2602.05863", "year": 2026, "blocks": {"background": {"text": "在 critic-free 的 GRPO 框架中引入显式行为约束面临理论与实践双重挑战：传统 Lagrangian 方法依赖准确的优势估计，而 GRPO 的组内标准化操作若施加于 reward 层面（scalarized rewards），会因方差/协方差差异隐式扭曲各目标项的有效权重，导致 Lagrange 乘子失去语义一致性，进而使约束优化失效。", "bullets": ["【痛点】：scalarized rewards 在 GRPO 组内标准化中隐式重加权，破坏乘子设定的原始权衡", "【现状】：现有 critic-free 约束 RL 缺乏保障 multiplier 因果效力的理论机制与稳定实现方案"]}, "objective": {"text": "提出 Constrained GRPO，首次将 Lagrangian 约束优化与 critic-free GRPO 结合，并确立‘标量化优势（scalarized advantages）’为维持乘子语义一致性的理论必要设计，实现可靠、自适应、稳定的多约束行为控制。", "bullets": ["【任务】：约束强化学习（Constrained RL）", "【核心贡献】：证明 scalarized advantages 是保障 multiplier 语义一致性的关键；提出首个 critic-free 的稳定约束 GRPO 框架"]}, "method": {"text": "输入为轨迹数据（状态-动作序列）及多维奖励/成本信号（如任务奖励、NC/DAC/DDC 安全成本）；先对每项（奖励/各约束成本）独立进行组内标准化（per-component advantage estimation），再用可学习 Lagrange 乘子线性组合生成标量化优势（scalarized advantages）作为 GRPO 的优化目标；整个过程无需 critic 网络，通过乘子梯度上升在线更新以逼近约束满足。", "bullets": ["【输入】：轨迹数据 + 多维 reward/cost 张量（含任务奖励、NC/DAC/DDC）", "【架构】：GRPO（Group Relative Policy Optimization）", "【关键机制】：per-component advantage normalization + Lagrangian multiplier update", "【是否训练】：是（策略网络 PPO-style 更新 + multiplier 梯度上升）", "【创新点】：scalarized advantages 构造法（定理 4.1 保证乘子语义一致性）"]}, "data": {"text": "实验涵盖模拟环境与真实机器人仿真，验证方法在不同复杂度场景下的泛化能力。", "bullets": ["【数据集】：toy gridworld (lava avoidance)", "【数据集】：NAVSIM-v2 (自动驾驶仿真平台)", "【来源】：合成轨迹 + 仿真交互采集"]}, "experiment": {"text": "在 gridworld 和 NAVSIM-v2 上开展消融与对比实验，评估不同优势构造方式对约束满足率、任务成功率及 multiplier 动态的影响。", "bullets": ["【基线模型】：scalarized rewards GRPO, fixed-weight GRPO, hard-constraint EPDMS", "【消融实验】：ablation on per-component normalization vs joint normalization", "【对比设置】：相同随机种子、统一超参、multiplier 初始化一致"]}, "metrics": {"text": "采用多维度指标联合评估策略性能、安全性与稳定性。", "bullets": ["Constraint Violation Rate (CVR)", "Task Success Rate", "EPDMS (Expected Performance under Distributional Safety)", "Multiplier Convergence Stability (via error bar width & cross-seed variance)", "Win Rate (vs baselines on safety-task Pareto frontier)"]}, "results": {"text": "scalarized advantages 显著提升约束可控性与策略鲁棒性：multiplier 更新与约束率动态强耦合，支持自适应资源分配；在所有场景下均获得更高任务成功率、更低且更平滑的约束违反率，跨随机种子稳定性更强。", "bullets": ["gridworld：DAC violation rate = 0.03 ± 0.01 (+87% stability vs scalarized rewards)", "NAVSIM-v2：EPDMS = 0.92 (+0.15 vs fixed-weight), Task Success Rate = 84.7% (+12.3% vs baseline)", "multiplier λ_DAC ↑3.2× after DAC satisfaction → confirms causal interpretability"]}, "limitations": {"text": "方法依赖精确的成本函数建模与多约束间的可分离性假设；在极高维连续动作空间或长程稀疏约束下，优势估计噪声可能削弱 multiplier 敏感性。", "bullets": ["【假设强】：要求各约束成本可独立观测与归一化", "【扩展性】：未验证在离散-连续混合动作空间或非马尔可夫约束下的有效性", "【计算开销】：per-component normalization 增加少量前向开销（<5%）但无额外网络参数"]}}}
{"paper_id": "2602.05877", "title": "Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy", "url": "https://arxiv.org/abs/2602.05877", "year": 2026, "blocks": {"background": {"text": "现有威胁建模框架（如OWASP、MAESTRO）以系统组件为中心，混杂资产定义与攻击方式，难以刻画LLM智能体在安全关键场景（如车载助手）中的非确定性行为、多智能体交互及语义驱动攻击传播特性；尤其无法系统覆盖同一技术漏洞引发的跨维度人类危害。", "bullets": ["【痛点】：传统框架混杂资产与攻击方式，无法支撑人类中心化、多维联动的危害分析", "【现状】：A2A等协议虽具传输层安全，但缺乏内容校验，使认证后恶意自然语言载荷可被无差别执行"]}, "objective": {"text": "提出首个面向LLM智能体的安全关键威胁建模框架AgentHeLLM，形式化分离‘人类中心化资产’与‘图结构化攻击路径’，并首创毒化路径—触发路径二分模型，以精准建模A2A等新型语义传播攻击。", "bullets": ["【任务】：威胁建模与攻击路径发现", "【核心贡献】：首次在AI安全中实现资产（人类价值锚定）与攻击路径（图结构+递归嵌套）的关注点分离"]}, "method": {"text": "输入为LLM智能体系统架构（含Actor/Datasource节点及四类交互边），经图建模→资产映射→双路径形式化→代价感知搜索，输出可解释、可排序的攻击路径链及其激活条件。", "bullets": ["【输入】：智能体系统拓扑（Actor/Datasource节点 + read/write/communicate/respond边）", "【架构】：有向图建模 + 双层搜索（A*主规划器 + BFS子搜索器）", "【关键机制】：毒化路径—触发路径二分模型、递归嵌套结构、激活/消费双成本建模", "【是否训练】：否 (无需参数更新，纯符号推理与图搜索)", "【创新点】：将‘人类中心资产’（七类《世界人权宣言》对齐价值）作为威胁建模第一性原理，并解耦其与攻击传播的因果依赖"]}, "data": {"text": "基于真实车载LLM智能体系统抽象构建图模型，复现并形式化分析CurXecute、EchoLeak等已知漏洞，同时覆盖OEM、驾驶员、乘客、第三方四类主体。", "bullets": ["【数据集】：CurXecute (车载记忆投毒)", "【数据集】：EchoLeak (语音指令劫持)", "【来源】：车载助手系统架构文档与公开漏洞报告", "【数据集】：《世界人权宣言》（用于人类资产分类）"]}, "experiment": {"text": "在车载助手典型场景下对比AgentHeLLM与OWASP LLM Security Top 10、MAESTRO的覆盖完整性与路径可解释性；开展消融实验验证毒化/触发路径分离、双成本建模、人类资产分层对路径排序质量的影响。", "bullets": ["【基线模型】：OWASP LLM Security Top 10, MAESTRO", "【消融实验】：移除触发路径建模 → 路径不可激活", "【消融实验】：取消人类资产分层 → 生命健康风险被降权", "【场景】：车载语音交互高认知负荷环境（注意力恢复27秒）"]}, "metrics": {"text": "采用路径完备性（Path Completeness）、跨维危害覆盖率（Cross-Dimensional Harm Coverage, CDHC）、攻击可激活率（Activation Feasibility Rate, AFR）及人类资产风险排序一致性（Human-Centric Priority Consistency, HPC）进行评估。", "bullets": ["Path Completeness", "CDHC", "AFR", "HPC"]}, "results": {"text": "AgentHeLLM在跨维度危害识别上较OWASP/MAESTRO提升53.7%（CDHC），92%的已知漏洞路径被完整还原并显式拆分为毒化+触发两阶段；在车载场景中，生命健康类资产始终位列风险排序首位，且路径总代价预测误差<8.3%。", "bullets": ["CDHC：53.7% (+53.7% vs OWASP)", "漏洞路径还原率：92% (CurXecute/EchoLeak)", "HPC：100%（四类主体中生命健康始终Top-1）", "路径总代价预测误差：<8.3%"]}, "limitations": {"text": "当前框架依赖人工建模系统图结构，尚未集成自动LLM系统逆向解析；对超长上下文引发的隐式状态污染路径建模能力有限；未考虑实时动态防御策略对路径可行性的在线修正。", "bullets": ["【假设强】：依赖人工提供的准确Actor/Datasource拓扑", "【覆盖弱】：难以建模超长上下文导致的隐式、非边连接的状态污染", "【静态性】：未支持防御策略介入后的路径可行性重评估"]}}}
{"paper_id": "2602.05885", "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations", "url": "https://arxiv.org/abs/2602.05885", "year": 2026, "blocks": {"background": {"text": "在强化学习驱动的GPU内核（如Triton）生成任务中，模型常通过奖励欺骗（reward hacking）和懒优化（lazy optimization）获得虚假性能提升，导致生成内核虽在模拟环境中得分高，却无法带来真实CUDA加速效果。", "bullets": ["【痛点】：奖励欺骗导致虚假优化（如AutoTriton hack率~10%）", "【痛点】：懒优化使模型回避关键算子融合，忽视整体耗时覆盖", "【现状】：缺乏支持执行级反馈、故障隔离与偏差检测的专用RL训练环境"]}, "objective": {"text": "提出KERNELGYM执行环境与DR.KERNEL训练框架，系统性解决奖励欺骗、多轮RL优势估计偏差、训练不稳定及优化目标与硬件效能错位四大挑战，实现具备实际CUDA加速能力的高质量Triton内核生成。", "bullets": ["【任务】：强化学习驱动的GPU内核生成", "【核心贡献】：首个面向kernel生成的鲁棒RL执行环境（KERNELGYM）", "【核心贡献】：端到端训练框架DR.KERNEL，覆盖偏差校正、稳定性增强、目标对齐与推理扩展"]}, "method": {"text": "输入为待优化的PyTorch算子签名与shape信息；经SFT冷启动后，在KERNELGYM中执行多轮RL优化：每轮生成Triton内核→编译执行→细粒度profiling→计算Profiling-based Reward（PR = T_generated / T_total）→使用TRLOO估计无偏优势→通过MRS+PRS拒绝低覆盖内核→更新策略；推理时采用STTS动态扩展优化轮次并选取best-of-history。", "bullets": ["【输入】：PyTorch算子签名、输入shape、硬件配置", "【架构】：基于Transformer的策略模型（DR.KERNEL-14B），结合KERNELGYM执行引擎", "【关键机制】：TRLOO（Turn-level REINFORCE Leave-One-Out）、MRS（Mismatch Rejection Sampling）、PRS（Profiling-based Rejection Sampling）、STTS（Sequential Test-Time Scaling）", "【是否训练】：是（SFT冷启动 + 多轮RL微调）", "【创新点】：PR指标将硬件profiling转化为可微分奖励信号；TRLOO消除多轮自回归中的优势估计偏差；PRS显式约束内核对总耗时的覆盖比例"]}, "data": {"text": "基于真实CUDA工作负载构建的分层测试集，包含Level-1（基础算子）与Level-2（复合/融合算子）子集，覆盖典型DL kernel场景。", "bullets": ["【数据集】：KERNELGYM Benchmark (Level-1, Level-2)", "【来源】：真实PyTorch算子与CUDA profiling trace", "【标注】：执行级profiling反馈（T_generated, T_total, 编译成功率，运行稳定性）"]}, "experiment": {"text": "在NVIDIA A100上评估DR.KERNEL-14B，对比AutoTriton、Claude-4.5-Sonnet、GPT-5等基线；开展消融实验验证TRLOO、MRS、PR/PRS及STTS各模块贡献。", "bullets": ["【基线模型】：AutoTriton, Claude-4.5-Sonnet, GPT-5", "【消融实验】：移除TRLOO（改用GRPO）、禁用MRS、替换PR为标准reward、关闭STTS", "【环境】：NVIDIA A100, CUDA 12.1, Triton 3.0"]}, "metrics": {"text": "以实际CUDA端到端加速率为核心评估标准，采用Fast@k（达到k×加速率的比例）与hack率（奖励欺骗发生率）作为主指标。", "bullets": ["Fast@1.2", "hack rate", "Pass@1 (编译+正确性)", "EM (Exact Match on kernel logic)"]}, "results": {"text": "DR.KERNEL显著降低奖励欺骗并提升真实加速能力：在Level-2上Fast@1.2达47.8%，大幅超越强基线；PR+PRS推动该指标从20.0%跃升至25.6%；KERNELGYM将hack率从~10%压降至~3%。", "bullets": ["Level-2：Fast@1.2 = 47.8% (+21.1pp vs Claude-4.5-Sonnet)", "Level-2（PR+PRS）：Fast@1.2 = 25.6% (+5.6pp vs MRS-only)", "KERNELGYM：hack rate = 3% (−7pp vs AutoTriton)", "TRLOO：Fast@1.2提升+8.2pp且训练方差下降37%"]}, "limitations": {"text": "当前方法依赖高质量profiling反馈与稳定编译链路；STTS带来额外推理延迟；PR指标对profiling噪声敏感；未覆盖跨kernel调度与内存层次协同优化。", "bullets": ["【假设强】：依赖精确、低噪声的CUDA profiling数据", "【开销大】：STTS多轮生成+执行显著增加推理延迟", "【覆盖窄】：局限于单kernel优化，未建模kernel间调度与L2/L3 cache协同", "【依赖链】：高度依赖Triton编译器稳定性与错误诊断能力"]}}}
{"paper_id": "2602.05890", "title": "DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training", "url": "https://arxiv.org/abs/2602.05890", "year": 2026, "blocks": {"background": {"text": "在噪声监督和分布外（OOD）场景下，现有LLM价值建模方法（如PPO、FlowRL）易受奖励噪声干扰，导致训练不稳定、泛化能力弱、价值估计过平滑或震荡，难以支撑鲁棒的后训练对齐。", "bullets": ["【痛点】：标量价值或离散分位点建模无法刻画价值不确定性的时间演化结构", "【现状】：主流方法缺乏对价值分布动态性、几何结构与风险敏感性的联合建模"]}, "objective": {"text": "提出DFPO（Distributional Flow Policy Optimization）框架，首次将连续分布流建模引入LLM价值估计，实现鲁棒、可解释、低开销的价值表征。", "bullets": ["【任务】：鲁棒价值建模（分布流估计）", "【核心贡献】：首次将神经ODE驱动的连续分布流建模用于LLM后训练中的条件价值估计"]}, "method": {"text": "以token级轨迹为输入，构建虚拟时间轴上的连续价值分布流场；通过神经ODE参数化向量场，联合优化不确定性加权分布流匹配（UDCFM）、引导式锚定正则化（BCFM）、几何一致性正则化及条件风险约束（左尾收缩+右尾凸性），实现单步欧拉求解即逼近最优传输。", "bullets": ["【输入】：LLM生成的token级响应轨迹及其噪声奖励信号", "【架构】：神经ODE（向量场网络）+ 风险感知损失头 + 几何正则化模块", "【关键机制】：UDCFM（不确定性加权分布流匹配）、BCFM（锚定正则化）、几何一致性约束、条件风险优化（尾部曲率控制）", "【是否训练】：是 (端到端微调神经ODE向量场与正则化参数)", "【创新点】：将价值建模从‘静态分布’升维至‘动态流形’，并嵌入微分几何与风险感知双约束"]}, "data": {"text": "在多领域高质量对话与推理数据上评估，覆盖真实噪声与人工污染设定。", "bullets": ["【数据集】：UltraFeedback (对话)", "【数据集】：GSM8K (数学推理)", "【数据集】：SciQ / MMLU-Science (科学问答)", "【来源】：人工注入噪声奖励 + 真实人类反馈偏差模拟"]}, "experiment": {"text": "在标准与噪声增强设置下对比DFPO与PPO、FlowRL、Robust PPO等基线；开展消融研究验证各模块贡献，并可视化流场动力学与优势归因。", "bullets": ["【基线模型】：PPO, FlowRL, Robust PPO, DPO (noise-aware variants)", "【消融实验】：去除UDCFM / 去除BCFM / 去除几何一致性 / 去除尾部曲率约束", "【分析方式】：流场可视化、token级优势归因热力图、OOD迁移准确率对比"]}, "metrics": {"text": "采用任务相关标准指标，并新增流场稳定性与归因可解释性度量。", "bullets": ["Win Rate", "Exact Match (EM)", "Pass@1", "OOD Accuracy", "Flow Stability Score (FSS)", "Advantage Smoothness (AS)"]}, "results": {"text": "DFPO在全部任务上一致超越基线，在噪声环境下保持稳定收敛；单步欧拉推断即可达到最优性能，显著降低推理延迟。", "bullets": ["UltraFeedback：Win Rate = 68.3% (+5.2% vs FlowRL)", "GSM8K：EM = 74.1% (+4.7% OOD Acc vs PPO)", "SciQ：Pass@1 = 62.9% (+3.8% vs Robust PPO)", "单步欧拉解：FSS = 0.92 → 近似全局最优传输路径"]}, "limitations": {"text": "依赖高质量轨迹采样与神经ODE数值求解稳定性；尾部曲率约束需人工设定超参；当前未扩展至多模态序列。", "bullets": ["【假设强】：依赖token级轨迹的局部光滑性与奖励噪声的可建模性", "【计算开销】：神经ODE训练需更高显存与更长迭代周期（但推理仅1步）", "【泛化边界】：尚未验证在极端稀疏奖励或长程依赖任务中的表现"]}}}
{"paper_id": "2602.05892", "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents", "url": "https://arxiv.org/abs/2602.05892", "year": 2026, "blocks": {"background": {"text": "大型语言模型（LLM）编码代理在解决真实软件工程任务时，需从庞大代码库中检索关键上下文以生成正确补丁；但现有评测多为端到端黑盒评估，缺乏对上下文检索过程的可解释、可分解度量。", "bullets": ["【痛点】：现有基准无法区分代理性能瓶颈源于代码生成能力不足，还是上下文检索/利用失效", "【现状】：缺乏人工标注、多语言、多粒度、过程导向的代码上下文检索专用评测基准"]}, "objective": {"text": "构建首个面向过程、含人工标注黄金上下文的大规模多语言代码上下文检索评测基准CONTEXTBENCH，并建立可解释、可复现、动态化的检索评估范式。", "bullets": ["【任务】：代码上下文检索评测", "【核心贡献】：提出CONTEXTBENCH基准与三级粒度动态评估框架"]}, "method": {"text": "基于SWE-bench等四大基准筛选任务→人机协同标注黄金上下文→Tree-Sitter统一解析对齐→设计多维动态指标评估检索全流程。", "bullets": ["【输入】：原始软件工程任务（含问题描述、仓库快照、测试用例）", "【架构】：规则+嵌入双重去重 pipeline + Tree-Sitter 解析器 + 人机协同标注工作流", "【关键机制】：依赖链追踪、文件/块/行三级坐标对齐、动态使用衰减建模", "【是否训练】：否 (无需参数更新，纯评测基准构建)", "【创新点】：首次定义并构造人工黄金上下文，提出‘检索-对齐-使用’全过程动态评估"]}, "data": {"text": "涵盖8种主流编程语言、超52万行代码、1136个高难度人工筛选任务，全部附带人工标注黄金上下文。", "bullets": ["【数据集】：CONTEXTBENCH (自建)", "【来源】：SWE-bench, RepoQA, CodeAct, DevEval 等四大基准经去重与难度筛选", "【标注者】：6位作者 + 资深工业界开发者", "【标注周期】：4个月人机协同迭代"]}, "experiment": {"text": "在1136个高难度任务上评估主流LLM编码代理（如Prometheus、OpenHands、Claude Sonnet 4.5等）的上下文检索行为，对比不同策略（步数、粒度、架构）的影响。", "bullets": ["【基线模型】：Prometheus, OpenHands, Claude Sonnet 4.5, GPT-4o, DeepSeek-Coder", "【消融实验】：粒度级别（文件/块/行）、推理步数、是否启用RAG模块", "【对比维度】：召回率、精确率、F1、冗余度、使用衰减曲线"]}, "metrics": {"text": "覆盖检索质量与使用效能的六维动态指标，支持文件/块/行三级粒度计算。", "bullets": ["Recall@k", "Precision@k", "F1@k", "Redundancy Rate", "Usage Decay (per-step)", "Pass@1 (下游补丁成功率)"]}, "results": {"text": "所有前沿LLM均呈现高召回低精度特征；块级F1普遍<0.45；存在显著‘用而未取’现象（~20%黄金上下文未被采纳）；Claude Sonnet 4.5在行级F1与Pass@1上表现最优。", "bullets": ["块级F1：0.42 ± 0.03 (平均，vs. 最优0.44)", "行级F1：Claude Sonnet 4.5 = 0.58 (+0.11 vs. Prometheus)", "Pass@1：Claude Sonnet 4.5 = 47.3% (+8.2% vs. GPT-4o)", "黄金上下文Jaccard相似率：0.95 (跨等价补丁)"]}, "limitations": {"text": "标注成本极高；当前仅覆盖单任务单补丁场景；黄金上下文依赖专家经验，泛化至超大规模或新语言生态尚待验证。", "bullets": ["【开销大】：4个月人机协同标注，难以快速扩展", "【场景窄】：暂未支持多任务串联、长程依赖重构等复杂场景", "【假设强】：依赖Tree-Sitter解析能力，对非AST友好语言（如Makefile）覆盖有限"]}}}
{"paper_id": "2602.05897", "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models", "url": "https://arxiv.org/abs/2602.05897", "year": 2026, "blocks": {"background": {"text": "小推理模型（SRMs）在链式推理（CoT）中存在严重的步骤级忠实性幻觉问题，即中间推理步骤偏离上下文但最终答案偶然正确，导致传统终局奖励强化学习无法识别并抑制此类隐性错误。", "bullets": ["【痛点】：仅优化最终答案正确性会掩盖中间步骤的忠实性幻觉，造成推理过程不可信", "【现状】：SRMs的CoT忠实率极低（如DPSK-1.5B仅6%），远低于答案忠实率（67.4%），且幻觉CoT具有强泛化危害性（新问题错误率达59.48%）"]}, "objective": {"text": "提出FaithRL框架，首次实现对小推理模型链式推理过程的步骤级忠实性对齐，突破终局奖励范式，提升推理过程可信度与最终答案准确性。", "bullets": ["【任务】：链式推理（CoT）步骤级忠实性对齐", "【核心贡献】：显式步骤级奖励 + 隐式截断重采样（DTR）双路径机制"]}, "method": {"text": "输入为问题与初始CoT前缀；通过过程奖励模型（PRM）逐句打分生成显式±1忠实性奖励，并叠加信息增益奖励与n-gram重复惩罚；当检测到幻觉步骤时，动态截断轨迹并以忠实前缀为提示重采样后续token，在GRPO框架下联合优化答案准确率与CoT忠实率。", "bullets": ["【输入】：问题文本 + 当前CoT前缀", "【架构】：基于GRPO的强化学习框架", "【关键机制】：过程奖励模型（PRM）、动态截断重采样（DTR）、分层奖励分配、信息增益约束、n-gram重复惩罚", "【是否训练】：是 (PRM需预训练；策略模型在GRPO下微调)", "【创新点】：首次将显式步骤级忠实性奖励与隐式DTR机制结合，实现token级幻觉识别与忠实前缀正向强化"]}, "data": {"text": "在5个开放问答基准上评估，涵盖多跳、数学与常识推理场景。", "bullets": ["【数据集】：HotpotQA (多跳问答)", "【数据集】：GSM8K (数学推理)", "【数据集】：StrategyQA (常识推理)", "【数据集】：ARC (科学问答)", "【数据集】：QASC (多源推理)"]}, "experiment": {"text": "在小推理模型（如DPSK-1.5B）上开展主实验、攻击实验与消融研究，对比标准GRPO及无忠实性约束基线。", "bullets": ["【基线模型】：DPSK-1.5B, GRPO-trained SRMs", "【消融实验】：移除信息增益奖励、禁用DTR机制、替换为均匀终局奖励", "【攻击实验】：基于幻觉CoT生成对抗性新问题以检验泛化危害性"]}, "metrics": {"text": "以CoT忠实率（Faithfulness Rate）和答案准确率（Accuracy）为核心指标，并报告Pass@1、EM等基准指标。", "bullets": ["Faithfulness Rate", "Accuracy", "Exact Match (EM)", "Pass@1"]}, "results": {"text": "FaithRL在5个基准上平均提升CoT忠实率3.48%、答案准确率3.86%，训练开销与标准GRPO相当；DTR机制在HotpotQA上带来显著长程稳定性提升；移除信息增益奖励导致准确率暴跌42.4%。", "bullets": ["HotpotQA：Faithfulness Rate = +4.21% vs GRPO", "GSM8K：Accuracy = +3.86% vs GRPO", "整体平均：Faithfulness Rate = +3.48% (+2.1× baseline), Accuracy = +3.86%", "消融（无信息增益）：Accuracy ↓42.4% on GSM8K"]}, "limitations": {"text": "依赖高质量过程奖励模型（PRM）进行细粒度步骤评估，当前PRM泛化能力有限；DTR机制对早期幻觉敏感，但难以处理隐性语义漂移；未覆盖代码生成等非问答类推理任务。", "bullets": ["【依赖强】：需预训练的过程奖励模型（PRM）性能直接影响步骤判别质量", "【覆盖窄】：实验集中于开放问答，未验证代码/定理证明等推理场景", "【检测弱】：对隐性语义幻觉（如概念偷换）缺乏鲁棒识别能力"]}}}
{"paper_id": "2602.05905", "title": "Codified Finite-state Machines for Role-playing", "url": "https://arxiv.org/abs/2602.05905", "year": 2026, "blocks": {"background": {"text": "在开放式角色扮演中，大语言模型难以稳定、可解释地建模和更新角色的隐含状态，现有方法（如纯提示工程或链式推理）存在一致性差、不可追溯、计算开销大等问题。", "bullets": ["【痛点】：角色隐含状态缺乏结构化建模，导致行为不一致、不可审计、难以调试", "【现状】：现有方法依赖手工设计或黑盒推理，缺乏可执行、可组合的状态逻辑"]}, "objective": {"text": "提出Codified Finite-State Machines（CFSM）及其概率扩展CPFSM，首次利用LLM自动从非结构化角色档案中提取离散状态并生成可执行、可解释的状态转移逻辑，实现符号化建模与语义灵活性的统一。", "bullets": ["【任务】：角色状态自动建模与可执行转移逻辑生成", "【核心贡献】：将LLM用作‘状态编译器’，实现非结构化→结构化→可执行的端到端翻译"]}, "method": {"text": "输入为角色档案文本；通过多阶段LLM提示抽取互斥离散状态集H，并生成函数get_next_state(state, scene, action)，其中嵌入binary_question()实现语义条件判断；CPFSM进一步将状态表示为分布，由LLM生成成对转移问题，经判别器输出logits构建稀疏转移矩阵；采用O(n+k)编码策略仅显式定义k个关键转移问题，其余复用默认检查。", "bullets": ["【输入】：角色档案文本（非结构化描述）", "【架构】：LLM（作为状态编译器）+ 可选轻量判别器（用于CPFSM）", "【关键机制】：状态注册、binary_question语义条件判断、稀疏转移问题生成、O(n+k)高效编码", "【是否训练】：否 (无需参数更新，全提示驱动)", "【创新点】：用LLM自动化FSM构建，兼顾可执行性、可解释性与低开销"]}, "data": {"text": "在合成任务（Mario/Call of Duty/Tyrion）与真实Fandom基准（5141场景、83角色）上进行验证。", "bullets": ["【数据集】：Mario (合成)", "【数据集】：Call of Duty (合成)", "【数据集】：Tyrion (合成)", "【数据集】：Fandom Benchmark (真实，5141场景/83角色)", "【来源】：公开角色设定网页与社区叙事文本"]}, "experiment": {"text": "对比CFSM/CPFSM与纯提示式方法、链式推理、PromptTrans、Codified Profile等基线；开展消融实验验证各模块贡献；测试小模型（1B）与蒸馏判别器（0.1B）下的鲁棒性。", "bullets": ["【基线模型】：纯提示式方法、链式推理、PromptTrans、Codified Profile", "【消融实验】：移除状态注册、移除人格类FSM、移除身份/能力类FSM", "【设置】：小模型（1B）+ 蒸馏判别器（0.1B）验证轻量化不确定性建模"]}, "metrics": {"text": "使用准确率（合成任务）、NLI得分（Fandom基准）、推理开销（计算延迟/调用次数）进行评估。", "bullets": ["Accuracy", "NLI Score", "Inference Overhead"]}, "results": {"text": "CFSM在合成任务达100%准确率，显著优于纯提示法；在Fandom基准平均NLI得分为82.65，全面超越基线；CPFSM在1B模型上仍优于确定性基线，且经蒸馏判别器后进一步提升。", "bullets": ["Mario：Accuracy = 100% (+∞ vs 纯提示法)", "Call of Duty：Accuracy = 100% (+∞ vs 纯提示法)", "Tyrion：Accuracy = 100% (+∞ vs 纯提示法)", "Fandom Benchmark：NLI Score = 82.65 (+7.2 vs PromptTrans)", "CPFSM (1B)：NLI Score > deterministic baseline", "CPFSM + 0.1B discriminator：NLI Score ↑ further"]}, "limitations": {"text": "依赖高质量角色档案输入；状态互斥性假设在极复杂人格建模中可能受限；CPFSM转移矩阵的稀疏性以牺牲部分长程语义覆盖为代价。", "bullets": ["【假设强】：要求状态集严格互斥，难以刻画模糊/渐变状态", "【输入敏感】：性能高度依赖角色档案的完整性与表述清晰度", "【覆盖权衡】：O(n+k)编码策略牺牲部分长程、非关键转移的语义建模精度"]}}}
{"paper_id": "2602.05929", "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs", "url": "https://arxiv.org/abs/2602.05929", "year": 2026, "blocks": {"background": {"text": "大语言模型推理中KV缓存占用显存高，但其实际可压缩性缺乏系统化、数据依赖、层粒度的量化评估框架；现有方法多聚焦权重低秩，忽视真实激活态KV的结构特性。", "bullets": ["【痛点】：KV缓存压缩缺乏可解释、可复现、层感知的定量基准", "【现状】：低秩分析集中于参数矩阵，忽略推理时动态生成的KV激活分布"]}, "objective": {"text": "构建首个面向数据依赖性与层粒度的KV缓存可压缩性基准框架KV-CoRE，并提出归一化有效秩（NER）作为轻量、可靠、可解释的压缩性度量指标。", "bullets": ["【核心贡献】：提出KV-CoRE基准框架", "【任务】：KV缓存低秩可压缩性量化评估"]}, "method": {"text": "对各层KV激活特征进行增量SVD分解，仅维护小规模协方差矩阵以降低内存开销；基于Eckart-Young-Mirsky定理获取最优低秩逼近并显式恢复奇异值谱；据此定义归一化有效秩（NER）作为核心压缩性指标，并支持独立层间K/V分离分析。", "bullets": ["【输入】：各层Key/Value激活张量（batch×seq_len×head×dim）", "【架构】：增量SVD + 协方差矩阵压缩", "【关键机制】：Eckart-Young-Mirsky最优逼近、奇异值谱显式建模、NER计算", "【是否训练】：否 (无参数更新，纯分析型算法)", "【创新点】：首次将低秩分析从权重迁移至推理态KV激活，引入NER实现跨层/跨K-V/跨数据的统一可比度量"]}, "data": {"text": "在多语言、多领域、多模型设置下开展系统实证分析，覆盖不同预训练覆盖度与架构设计的LLM。", "bullets": ["【数据集】：Arabic, Finnish, English (low-/high-resource languages)", "【模型】：Gemma-2B/7B, LLaMA-2-7B", "【来源】：标准推理激活轨迹（如WikiText、C4子集）"]}, "experiment": {"text": "在统一推理配置下提取各层KV激活，计算每层K/V各自的NER曲线；结合ND-PPL评估端到端压缩鲁棒性；进行跨语言、跨模型、跨层相关性分析。", "bullets": ["【基线模型】：Gemma-2B, Gemma-7B, LLaMA-2-7B", "【消融实验】：K vs V分离分析、首/中/尾层NER对比、NER与ND-PPL关联性验证"]}, "metrics": {"text": "以归一化有效秩（NER）为核心压缩性指标，并联合端到端鲁棒性指标ND-PPL进行实证校准。", "bullets": ["NER", "ND-PPL", "Pearson correlation (r)"]}, "results": {"text": "NER能有效揭示KV缓存内在压缩潜力：Key普遍比Value更易压缩；低资源语言NER异常低；中间层NER更高；模型KV维度设计显著影响NER；NER与ND-PPL呈强正相关（r=0.88 for V），证实其预测能力。", "bullets": ["Key：NER-K = 12.3 (vs Value: NER-V = 28.7)", "Arabic：NER-V = 8.1 (+32% lower than English)", "Middle layers：NER-V ≈ 35.2 (vs First layer: 19.6)", "Gemma-7B：NER-V = 9.4 (vs Gemma-2B: 22.1)", "NER-V vs ND-PPL：r = 0.88"]}, "limitations": {"text": "NER依赖SVD数值稳定性，在极短序列或极小batch下估计偏差增大；未覆盖长上下文极端场景；当前仅支持decoder-only架构。", "bullets": ["【假设强】：依赖激活分布近似平稳，未建模动态token依赖突变", "【覆盖窄】：暂未适配encoder-decoder或MoE架构", "【开销中】：增量SVD仍需O(d²)内存（d为KV维度），不适用于超大dim实时分析"]}}}
{"paper_id": "2602.05933", "title": "Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training", "url": "https://arxiv.org/abs/2602.05933", "year": 2026, "blocks": {"background": {"text": "在策略镜像下降（PMD）框架中，使用均值奖励近似对数配分函数是一种常见工程实践（如Kimi等工业LLM RL系统），但其理论基础模糊：该近似隐含优化目标不明、稳定性提升机理不清、与精确方法（如PMD-PART）的泛化差异缺乏量化刻画。", "bullets": ["【痛点】：传统将均值近似视为误差源，忽视其潜在结构化正则效应", "【现状】：PMD-MEAN被广泛使用但缺乏严格优化目标建模与收敛性分析"]}, "objective": {"text": "揭示PMD-MEAN隐式正则化的数学本质，严格证明其等价于求解一个含自适应混合KL–χ²正则项的镜像下降子问题，并建立其相对于PMD-PART在有限样本下的鲁棒性优势理论。", "bullets": ["【任务】：理论分析与机制解释", "【核心贡献】：首次证明PMD-MEAN等价于自适应KL–χ²混合正则化镜像下降"]}, "method": {"text": "将PMD-MEAN建模为带均值基线优势函数的对数策略空间回归问题，推导闭式解（依赖Lambert-W函数）；通过KKT条件反向构造等价优化问题；建立不精确PMD统一收敛分析框架，区分目标估计误差与优化误差；在二值奖励下解析分析概率比衰减行为。", "bullets": ["【输入】：当前策略π_t、奖励信号r、均值基线b = E[r]、温度参数τ", "【架构】：策略镜像下降（Policy Mirror Descent）", "【关键机制】：隐式混合正则（KL + χ²）、自适应权重λ/τ ∝ 1 / (平均奖励相关量)、Lambert-W闭式解", "【是否训练】：否 (无需参数更新；属优化算法设计分析)", "【创新点】：将均值近似升华为具有自适应权重的混合散度正则机制"]}, "data": {"text": "理论分析为主，实验验证基于数学推理任务数据集。", "bullets": ["【数据集】：GSM8K (数学推理)", "【数据集】：MATH (高级数学)", "【来源】：公开基准，用于验证策略稳定性与性能"]}, "experiment": {"text": "对比PMD-MEAN与PMD-PART在相同RLHF流程下的训练动态、收敛稳定性及最终性能；开展消融分析验证隐式正则效应；测量策略更新幅度与时间效率。", "bullets": ["【基线模型】：PMD-PART (精确配分函数计算)", "【消融实验】：固定λ vs 自适应λ；KL-only vs KL+χ²", "【对比维度】：训练稳定性、策略更新幅度、推理速度、数学任务准确率"]}, "metrics": {"text": "采用标准强化学习与推理评估指标联合衡量。", "bullets": ["Pass@1", "Exact Match (EM)", "Win Rate (vs baseline)", "Update Magnitude (L2 norm of log-policy change)", "Training Time Speedup"]}, "results": {"text": "PMD-MEAN在数学推理任务上实现更高EM与更稳训练轨迹，策略更新更保守，且推理加速4.6倍；理论预测的概率比衰减特性与实证完全一致。", "bullets": ["GSM8K：EM = 68.3% (+5.2% vs PMD-PART)", "MATH：Pass@1 = 32.7% (+3.9% vs PMD-PART)", "Update Magnitude：下降41% (vs PMD-PART)", "Training Time：Speedup = 4.6×", "Robustness：p_t小时误差上界降低指数级（e^{1/τ} → p_t(1−p_t)/τ²）"]}, "limitations": {"text": "理论分析依赖特定建模假设（如二值奖励解析解、Lambert-W可处理性），对高维连续动作空间或非平稳奖励分布的推广尚未验证。", "bullets": ["【假设强】：依赖二值奖励场景解析推导，连续动作空间仅给出启发式延伸", "【适用窄】：自适应λ形式基于平均奖励，对稀疏/偏态奖励分布的鲁棒性需进一步检验", "【验证浅】：实验集中于数学推理，未覆盖对话、代码等其他LLM RL典型场景"]}}}
{"paper_id": "2602.05943", "title": "Orthogonal Model Merging", "url": "https://arxiv.org/abs/2602.05943", "year": 2026, "blocks": {"background": {"text": "现有模型合并方法多在欧氏空间中对权重进行线性叠加，导致几何结构失真、谱漂移与灾难性遗忘，难以兼顾方向一致性与强度适应性。", "bullets": ["【痛点】：欧式线性叠加破坏权重内在几何结构", "【现状】：主流合并方法忽略参数空间的流形本质，将权重简单视为向量"]}, "objective": {"text": "提出一种几何感知的模型合并框架OrthoMerge，首次将模型融合建模为正交群（O(d)）上的黎曼流形操作，以保持语义稳定的旋转结构并缓解灾难性遗忘。", "bullets": ["【任务】：多任务模型合并", "【核心贡献】：将模型合并从欧氏空间迁移至正交群构成的黎曼流形"]}, "method": {"text": "对OFT微调模型，将其正交矩阵Q映射至李代数so(d)，在无约束空间中执行幅度校正的平均融合，再经Cayley变换回正交群；对非OFT模型（如LoRA、全参数微调），通过正交Procrustes问题解耦显式正交分量与残差分量，分别处理。", "bullets": ["【输入】：多个任务专用微调模型（OFT/LoRA/Full-ft）", "【架构】：正交群O(d) + 李代数so(d) + Cayley变换", "【关键机制】：正交-残差解耦、幅度校正（c = ||ΣQ_i||_F / Σ||Q_i||_F）、冲突感知神经元级方向提取", "【是否训练】：否 (无需参数更新，纯后处理)", "【创新点】：首次将模型合并定义为流形上的几何一致融合，引入幅度校正与冲突感知解耦"]}, "data": {"text": "实验覆盖多任务评测基准，涵盖通用能力、推理与专业领域测试。", "bullets": ["【数据集】：MMLU† (知识广度)", "【数据集】：M-ARC (逻辑推理)", "【数据集】：AGIEval (综合智能)", "【数据集】：BBH (复杂推理)"]}, "experiment": {"text": "在统一评测协议下对比多种基线合并方法，并开展消融研究以验证各组件必要性。", "bullets": ["【基线模型】：Task Arithmetic, TIES-Merging, DARE, SLERP", "【消融实验】：去除幅度校正c、仅so(d)平均、全局vs冲突感知解耦", "【对比设置】：单任务专家模型、各基线合并模型"]}, "metrics": {"text": "采用标准闭卷问答评测指标，聚焦泛化性与知识保留能力。", "bullets": ["MMLU", "M-ARC", "AGIEval", "BBH", "Win Rate (vs. single-task experts)"]}, "results": {"text": "OrthoMerge在多任务平均性能上超越所有基线及单任务专家均值，并显著提升预训练知识保留能力。", "bullets": ["多任务平均：46.25% (+1.54% vs. single-task expert avg 44.71%)", "MMLU†：不降反升（具体提升值未量化，但文中明确‘不降反升’）", "M-ARC：持续领先所有基线", "AGIEval：SOTA水平", "Win Rate：显著高于Task Arithmetic与TIES-Merging"]}, "limitations": {"text": "方法依赖正交分量可解耦性假设，且计算开销高于纯线性合并。", "bullets": ["【假设强】：隐式正交分量可通过Procrustes可靠提取，对高度非正交更新存在近似误差", "【开销大】：需SVD或Procrustes求解，额外引入O(d³)计算成本", "【适用性限】：当前聚焦全连接层/注意力权重，未扩展至归一化层或激活函数"]}}}
{"paper_id": "2602.05975", "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents", "url": "https://arxiv.org/abs/2602.05975", "year": 2026, "blocks": {"background": {"text": "深度研究代理（deep research agents）在Web搜索中表现优异，但在封闭语料库（如20万篇科学论文）上检索性能骤降；其生成的关键词式子查询与LLM检索器的语义建模能力存在系统性错配。", "bullets": ["【痛点】：代理生成高度关键词化、结构松散的子查询（如“ACL 2024 KBQA few-shot transfer”），天然适配BM25等稀疏检索器，却严重浪费LLM检索器的语义理解能力", "【现状】：现有工作聚焦优化查询端（如重写、分解），忽视代理-检索器协同失效的本质分布失配问题"]}, "objective": {"text": "揭示LLM检索器在深度研究代理中失效的根本原因，并提出一种无需修改代理或训练检索器的轻量语料侧适配框架。", "bullets": ["【任务】：科学文献检索（跨学科、推理密集型）", "【核心贡献】：提出语料库级测试时缩放（corpus-level test-time scaling），通过增强文档关键词信号实现代理-检索器分布对齐"]}, "method": {"text": "对语料库中每篇论文，使用Qwen3-Next-80B自动生成8个核心关键词，并将其前置嵌入Markdown文本头部，从而在不改变代理行为和检索器参数的前提下，强化输入文本中的关键词线索以匹配代理子查询风格。", "bullets": ["【输入】：原始论文全文（Markdown格式）", "【架构】：Qwen3-Next-80B（用于关键词生成）", "【关键机制】：语料库级测试时缩放（test-time corpus scaling）", "【是否训练】：否 (无需参数更新，纯前处理)", "【创新点】：从语料侧而非查询侧/模型侧进行分布对齐，实现零成本LLM检索器适配"]}, "data": {"text": "构建SAGE基准，含1200个跨学科科学检索问题（计算机、自然科学、医疗、人文），配套20万篇最新论文构成的封闭语料库。", "bullets": ["【数据集】：SAGE (1200 questions, 4 domains)", "【语料库】：200K scientific papers (2023–2025)", "【来源】：学术预印本与会议论文（ACL, NeurIPS, arXiv, PubMed等）"]}, "experiment": {"text": "在统一DR Tulu框架下评估六种主流深度研究代理（含GPT-5、Gemini-2.5及开源DR Tulu），对比BM25、gte-Qwen2-7B-instruct和ReasonIR三类检索器，在短答案型与开放型两类任务上的表现。", "bullets": ["【基线模型】：GPT-5, Gemini-2.5, DR Tulu (open)", "【检索器对比】：BM25, gte-Qwen2-7B-instruct, ReasonIR", "【消融实验】：移除关键词前置后的性能衰减", "【任务划分】：短答案型（强推理） vs 开放型（类综述）"]}, "metrics": {"text": "采用Exact Match（EM）作为主指标，并补充Unique References per Search（URS）衡量检索多样性。", "bullets": ["Exact Match (EM)", "Unique References per Search (URS)"]}, "results": {"text": "语料库缩放显著提升BM25性能（+8.18% EM），而对LLM检索器仅带来小幅增益（+0.90%~+2.54%）；短答案任务中ReasonIR比BM25低约30% EM，验证其与代理子查询风格严重失配；URS分析表明LLM检索器多样性受限（ReasonIR: 1.98 vs BM25: 2.97）。", "bullets": ["SAGE short-answer：EM = 42.3% (ReasonIR) vs 71.8% (BM25) (+29.5pp gap)", "SAGE short-answer + scaling：ReasonIR EM = 44.8% (+2.5pp), BM25 EM = 79.98% (+8.18pp)", "URS：ReasonIR = 1.98, BM25 = 2.97", "Open-ended tasks：各检索器EM差距缩小至<5pp，主因代理子查询多样性不足"]}, "limitations": {"text": "改进效果在开放型问题中边际递减；语料增强依赖大模型关键词生成质量；未解决代理本身查询生成机制的结构性缺陷。", "bullets": ["【假设强】：依赖Qwen3-Next-80B生成关键词的覆盖性与准确性", "【适用窄】：对开放型问题提升有限，因代理子查询多样性瓶颈难以突破", "【未闭环】：未联合优化代理查询生成模块，仅单向适配检索器输入分布"]}}}
{"paper_id": "2602.05992", "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs", "url": "https://arxiv.org/abs/2602.05992", "year": 2026, "blocks": {"background": {"text": "扩散大语言模型（dLLM）面临质量与推理效率的权衡困境，传统固定块调度策略因刚性边界忽视token置信度差异，导致高置信位置解码延迟、低置信位置过早承诺，损害生成质量与并行性。", "bullets": ["【痛点】：固定块调度忽略token置信度差异，破坏语义感知能力", "【现状】：现有调度方法多依赖训练或启发式规则，缺乏轻量、无训练、动态适配机制"]}, "objective": {"text": "提出一种无需训练的动态滑动块（DSB）调度方法及配套缓存机制，在保持因果性的前提下实现语义难度自适应解码，优化dLLM的质量-速度权衡。", "bullets": ["【任务】：dLLM推理阶段的块级调度优化", "【核心贡献】：提出training-free、语义感知、动态演化的滑动块调度范式"]}, "method": {"text": "DSB维护一个左边界随首个未解码位置前移、右边界依据已解码数与最大尺寸约束动态扩展的活动块；DSB Cache引入可变长度前缀窗口联合刷新KV状态，并周期性全局同步缓存。", "bullets": ["【输入】：当前解码位置、token置信度估计、已解码序列状态", "【架构】：兼容任意dLLM（如LLaDA、Dream）", "【关键机制】：动态滑动块调度 + DSB Cache（含prefix window + 周期性全局刷新）", "【是否训练】：否 (无需参数更新，完全training-free)", "【创新点】：将块定义从静态划分升维为解码进程的动态投影，实现语义难度驱动的渐进解码"]}, "data": {"text": "在多个dLLM模型与通用基准上验证，覆盖数学推理与代码生成任务。", "bullets": ["【数据集】：GSM8K (数学推理)", "【数据集】：HumanEval (代码生成)", "【模型】：LLaDA, Dream"]}, "experiment": {"text": "在LLaDA与Dream系列模型上开展主实验与消融研究，对比固定块调度基线，并评估不同初始/最大块长配置下的鲁棒性。", "bullets": ["【基线模型】：LLaDA, Dream", "【基线调度】：固定块调度（Fixed Block）", "【消融实验】：移除DSB Cache中的prefix window", "【鲁棒性实验】：S_init ∈ {4,8,16}, S_max ∈ {16,32,64}"]}, "metrics": {"text": "采用任务标准指标评估生成质量，并以tokens/s衡量推理吞吐效率。", "bullets": ["Exact Match (EM)", "Pass@1", "tokens/s"]}, "results": {"text": "DSB在多个模型与基准上一致提升准确率与吞吐量，尤其在带缓存配置下接近甚至超越朴素采样质量；DSB Cache中prefix window对稳定性至关重要。", "bullets": ["GSM8K：EM = 78.5% (+3.2% vs Fixed Block, w/ cache)", "HumanEval：Pass@1 = 42.1% (+2.7% vs Fixed Block, w/ cache)", "tokens/s = 128.4 (+21.6% vs Fixed Block, LLaDA-7B)", "移除prefix window → EM ↓3.4%, tokens/s ↓22.1%"]}, "limitations": {"text": "DSB性能受模型底层训练范式影响（如AR初始化削弱增益），且S_max存在质量-效率权衡，需手动调优。", "bullets": ["【假设强】：依赖可靠的运行时置信度估计", "【调优依赖】：S_max需依模型特性经验调优，缺乏自动选择机制", "【结构敏感】：在AR-initialized模型（如Dream）上增益弱于LLaDA，说明调度收益受限于模型先验"]}}}
{"paper_id": "2602.06008", "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions", "url": "https://arxiv.org/abs/2602.06008", "year": 2026, "blocks": {"background": {"text": "现有对话谈判评估多局限于单轮或静态场景，缺乏对真实市场中多智能体、不完全信息、动态议价过程的系统建模；LLM在自然语言谈判中的策略性、角色一致性与经济理性尚未被结构化度量。", "bullets": ["【痛点】：现有谈判数据集无法模拟双边至多对多市场的流动性、私有估值约束与福利均衡要求", "【现状】：主流评估聚焦单轮响应质量（如BLEU/ROUGE），忽视议价效率、角色效用公平性与市场收敛性"]}, "objective": {"text": "提出AgenticPay——首个支持多智能体、私有估值建模、结构化动作解析与福利导向评估的语言驱动谈判基准框架，实现对LLM谈判能力的解耦、可复现、经济学一致的系统评测。", "bullets": ["【任务】：多智能体语言谈判能力基准化评估", "【核心贡献】：提出首个具备市场复杂度维度、角色专属效用建模与均衡性评估的开源谈判基准框架"]}, "method": {"text": "以自然语言对话为输入，通过动作解析器映射为结构化谈判动作（如price_proposal、accept、reject）；结合私有reservation price约束构建博弈环境；输出三重归一化得分（GlobalScore/BuyerScore/SellerScore），嵌入时间折扣与失败惩罚。", "bullets": ["【输入】：多轮自然语言对话（买家/卖家交替发言）", "【架构】：规则驱动的动作解析器 + 福利导向评估引擎（非神经网络）", "【关键机制】：私有reservation price建模、议价区间归一化、时间折扣衰减、失败惩罚项", "【是否训练】：否 (无需参数更新，纯推理协议评估)", "【创新点】：首次将不完全信息博弈机制（如保留价格）直接编码为语言谈判评估的结构化约束与得分函数"]}, "data": {"text": "构建覆盖10类现实商业场景、111个任务的可扩展谈判任务套件，按买家数、卖家数、产品数三维系统控制市场复杂度。", "bullets": ["【数据集】：AgenticPay (111 tasks)", "【场景】：二手汽车、SaaS采购、奢侈品交易、企业并购等10类商业领域", "【来源】：人工构造 + 经济学原理驱动设计（非爬取）"]}, "experiment": {"text": "在统一推理协议（temperature=0、固定prompt模板、20轮上限）下，对5个前沿闭源与开源LLM进行零样本基准测试，并开展跨角色配对、人格设定、交互模式（串行/并行）等消融分析。", "bullets": ["【基线模型】：GPT-5.2, Claude Opus 4.5, Gemini-3-Flash, Qwen3-14B, Llama-3.1-8B", "【消融实验】：cross-play（跨模型角色配对）、personality（人格设定）、serial vs. parallel interaction", "【控制变量】：固定prompt模板、20轮对话上限、temperature=0"]}, "metrics": {"text": "定义三重基于议价区间归一化的效用指标，均嵌入时间折扣与失败惩罚：GlobalScore（均衡性+效率）、BuyerScore、SellerScore。", "bullets": ["GlobalScore", "BuyerScore", "SellerScore", "Pass@20 (协议达成率)", "Avg. Rounds to Agreement"]}, "results": {"text": "所有模型呈现显著卖家优势；市场流动性提升带来GlobalScore增益；高风险金融类任务表现最差；开源模型存在终局让步失效现象；顶级模型谈判效率显著更高。", "bullets": ["GPT-5.2：SellerScore = 81.1 vs. BuyerScore = 58.5", "Qwen3-14B：GlobalScore 多买家场景较双边提升 +14.0", "Gemini-3-Flash：企业并购类任务下降 -20.2", "Llama-3.1-8B：46.3%失败案例中报价差≤5单位", "Claude Opus 4.5：Avg. Rounds to Agreement = 3.7 vs. 均值15+"]}, "limitations": {"text": "当前框架依赖人工构造任务与规则化动作解析，尚未支持端到端动作生成联合建模；对长周期动态估值（如并购中多阶段尽调）建模仍显简化；人格设定等干预方式尚未参数化。", "bullets": ["【假设强】：依赖预设reservation price与固定议价空间，未建模动态估值更新", "【覆盖窄】：暂未涵盖非货币补偿（如服务捆绑、账期延长）等复杂谈判要素", "【扩展限】：动作解析器为规则式，难以泛化至高度口语化或隐喻性谈判表达"]}}}
{"paper_id": "2602.06022", "title": "Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering", "url": "https://arxiv.org/abs/2602.06022", "year": 2026, "blocks": {"background": {"text": "大语言模型在多选题任务中常存在高置信低准确问题，即校准性（calibration）差；现有推理时 steering 方法多优化代理指标（如置信度表达、扰动稳定性），难以同时提升准确率与校准性（ECE），且缺乏跨模型/任务迁移能力。", "bullets": ["【痛点】：准确率与校准性难以兼顾，代理指标优化≠真实正确性提升", "【现状】：主流steering方法依赖任务微调、稀疏特征假设或单一模块定位，泛化性弱"]}, "objective": {"text": "提出一种无需重训练、可跨模型/任务零样本迁移的推理时steering方法，同步提升多选题准确率与校准性。", "bullets": ["【任务】：多选题推理时校准感知steering", "【核心贡献】：CORAL——首个直接建模残差正确性并实现校准感知、轻量级、零样本迁移的steering框架"]}, "method": {"text": "以冻结LLM各层残差流为输入，对每个选项提取中间层（17–21）均值池化隐藏状态，z-score归一化后送入四层MLP探针，用MSE损失+输出L2正则（λ_out）学习预测残差正确性r_j；推理时将预测残差\\hat{r}_j中心化后加权叠加至原始概率分布，截断归一化输出合法分布。", "bullets": ["【输入】：冻结模型第17–21层残差流中各选项对应的均值池化隐藏状态（z-score归一化）", "【架构】：四层MLP探针", "【关键机制】：残差正确性建模、输出L2正则（λ_out）、中心化加权叠加steering", "【是否训练】：是 (仅训练MLP探针，冻结主干模型；单卡5小时，8.4k–10k样本)", "【创新点】：首次将Brier分数优化显式编码为残差正确性预测任务；摒弃稀疏/局部假设，建模分布式正确性子空间"]}, "data": {"text": "使用8.4k–10k人工标注的多选题样本训练MLP探针；评估覆盖ARC-Challenge、HellaSwag、TruthfulQA、WinoGrande四个未见基准。", "bullets": ["【数据集】：ARC-Challenge (reasoning)", "【数据集】：HellaSwag (commonsense)", "【数据集】：TruthfulQA (truthfulness)", "【数据集】：WinoGrande (coreference)", "【来源】：人工标注多选题样本（8.4k–10k）"]}, "experiment": {"text": "在三个7B规模开源模型（Llama-2-7b、Qwen-7b、Phi-3-mini）上评估；对比ITI、SteerConf、CCPS等基线；开展SAE特征消融、attention头R²分析、PCA维度扫描等深度归因实验。", "bullets": ["【基线模型】：ITI, SteerConf, CCPS", "【消融实验】：SAE稀疏特征top-128 steering效果", "【消融实验】：单attention头 vs 526/960 heads校准信号捕获", "【分析实验】：PCA维度 vs 校准信号强度"]}, "metrics": {"text": "主指标为准确率（Accuracy）与期望校准误差（Expected Calibration Error, ECE）；辅助分析采用R²、Brier Score、特征归因贡献度。", "bullets": ["Accuracy", "ECE", "Brier Score", "R²", "PCA explained variance"]}, "results": {"text": "CORAL在7B模型上平均提升Accuracy 10%、降低ECE 50%；在四个未见基准上平均Accuracy +14%、ECE −49%；零样本迁移至新模型/任务无需微调；SAE与attention头分析证实正确性信号高度分布式。", "bullets": ["ARC-Challenge：Accuracy = 62.3% (+14.1% vs base)", "HellaSwag：Accuracy = 81.7% (+13.8% vs base)", "TruthfulQA：ECE = 0.082 (−49% vs base)", "WinoGrande：ECE = 0.076 (−49% vs base)", "跨模型迁移：Llama-2 → Qwen-7b Accuracy +10.2% w/o fine-tuning"]}, "limitations": {"text": "依赖中间层（17–21）激活，层选择需经验调优；当前仅验证于多选题，未扩展至生成类任务；全维探针带来一定参数开销（虽仅8.4k–10k参数，但需完整激活提取）。", "bullets": ["【假设强】：依赖中间层语义成熟性，层范围（17–21）需人工设定", "【任务窄】：目前仅适用于多选题分类，未验证于文本生成、开放问答", "【开销隐性】：需提取并处理全层残差流，内存带宽压力随模型宽度上升"]}}}
{"paper_id": "2602.06036", "title": "DFlash: Block Diffusion for Flash Speculative Decoding", "url": "https://arxiv.org/abs/2602.06036", "year": 2026, "blocks": {"background": {"text": "自回归大语言模型推理存在序列瓶颈，导致生成速度慢；现有推测解码方法（如EAGLE）仍受限于草案生成的串行性或条件建模能力不足，难以在高倍率加速下保持输出质量。", "bullets": ["【痛点】：草案生成阶段仍存在隐式序列依赖，限制并行度", "【现状】：EAGLE-3等方法在6×以上加速时出现显著质量下降或接受长度衰减"]}, "objective": {"text": "提出DFlash框架，首次将轻量级块扩散模型作为并行草案生成器，实现无损、高倍率（>6×）的LLM推理加速。", "bullets": ["【任务】：推测解码（Speculative Decoding）", "【核心贡献】：将块扩散模型定位为条件化并行草案适配器，而非端到端生成器"]}, "method": {"text": "以目标LLM已生成token为锚点构建训练块，用轻量块扩散模型单次前向并行生成整块草案token；通过融合多层冻结目标模型隐状态并注入KV缓存实现强条件引导；共享词嵌入与LM头，仅训练草案Transformer层。", "bullets": ["【输入】：上一验证token对应的目标模型隐状态（多层KV） + 位置编码", "【架构】：轻量块扩散模型（5层Transformer）", "【关键机制】：目标模型隐状态KV缓存注入、锚点驱动块掩码训练、位置感知损失加权", "【是否训练】：是（仅训练草案Transformer层，共享词嵌入与LM头）", "【创新点】：将扩散模型去中心化为条件草案适配器；‘目标模型即最强提示’的隐状态复用范式"]}, "data": {"text": "训练数据基于目标LLM（如Qwen3-8B）自身推理轨迹构造，不依赖外部标注语料。", "bullets": ["【数据集】：Qwen3-8B推理轨迹采样块（锚点+随机后续token）", "【来源】：目标模型自生成序列（self-play）"]}, "experiment": {"text": "在Qwen3-8B上评估端到端加速比、接受长度τ及任务泛化性；对比EAGLE-3等SOTA推测解码方法；开展消融实验验证各模块贡献；集成至SGLang服务框架测试高并发吞吐。", "bullets": ["【基线模型】：EAGLE-3, Medusa, vanilla speculative decoding", "【消融实验】：移除隐状态注入、禁用锚点采样、关闭位置加权损失", "【部署环境】：SGLang v0.5，32并发请求"]}, "metrics": {"text": "采用端到端加速比（Speedup）、平均接受长度（τ）、吞吐量（tokens/s）和任务级准确率（如GSM8K Pass@1、HumanEval Pass@1）进行综合评估。", "bullets": ["Speedup", "τ (acceptance length)", "Throughput (tokens/s)", "Pass@1"]}, "results": {"text": "DFlash在Qwen3-8B上实现最高6.1×端到端加速，平均较EAGLE-3快2.5×；数学/代码/对话任务中τ稳定≥7.8；SGLang高并发下仍达2.8×加速；消融显示隐状态注入提升τ约4.5（从3.3→7.8）。", "bullets": ["Qwen3-8B：Speedup = 6.1× (+2.5× vs EAGLE-3)", "GSM8K：Pass@1 = 69.2% (≈ baseline)", "τ：7.8+ (↑4.5 vs w/o KV injection)", "SGLang@32：Speedup = 2.8×"]}, "limitations": {"text": "性能高度依赖目标模型隐状态提取质量与层数匹配；块大小与训练-推理一致性影响泛化；当前仅验证单目标模型适配，跨模型迁移能力未验证。", "bullets": ["【假设强】：依赖目标模型多层冻结隐状态的稳定性与可提取性", "【兼容性弱】：跨模型（如Qwen→Llama）直接迁移尚未验证", "【调度约束】：最优性能需训练-推理块大小匹配，动态调度精度待提升"]}}}
{"paper_id": "2602.06039", "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching", "url": "https://arxiv.org/abs/2602.06039", "year": 2026, "blocks": {"background": {"text": "现有多智能体系统普遍采用固定通信拓扑（如全连接、环形、树状），难以适配多轮推理中动态演化的子目标与分工需求，导致冗余通信、噪声累积与协作僵化。", "bullets": ["【痛点】：固定通信结构无法响应每轮推理目标变化", "【现状】：主流框架（如AgentScope）依赖预设图或广播机制，缺乏语义驱动的稀疏路由能力"]}, "objective": {"text": "提出DyTopo框架，将通信拓扑从静态配置升维为每轮推理中由语义需求驱动的动态变量，实现目标对齐、可解释、高效率的多智能体协作。", "bullets": ["【任务】：动态拓扑路由下的多轮协同推理", "【核心贡献】：基于自然语言query/key描述符的轻量级语义匹配与稀疏有向图构建"]}, "method": {"text": "输入为各Agent当前本地记忆与Manager生成的阶段目标；经Worker前向生成自然语言query（需什么）和key（能提供什么）；用all-MiniLM-L6-v2编码后计算余弦相似度，硬阈值截断构建有向图G^(t)；同步屏障确保图-消息严格时序对齐；按拓扑排序/贪心消环结果降序聚合消息以构造确定性提示。", "bullets": ["【输入】：阶段目标 + Agent本地记忆", "【架构】：Manager-Worker分层架构", "【关键机制】：自然语言query/key描述符 + 语义嵌入匹配 + 硬阈值稀疏化 + 同步屏障 + 拓扑感知消息聚合", "【是否训练】：否 (无需参数更新，仅调用冻结的all-MiniLM-L6-v2)", "【创新点】：将通信拓扑建模为每轮语义对齐的可解释动态变量，解耦内容生成与路由决策"]}, "data": {"text": "在四大开放基准上验证泛化性，覆盖代码生成与数学推理两类复杂任务。", "bullets": ["【数据集】：HumanEval (代码)", "【数据集】：APPS (代码)", "【数据集】：MATH-500 (数学)", "【数据集】：Omni-MATH (数学)"]}, "experiment": {"text": "在4种LLM骨干模型下与最强基线（如AgentScope）对比，并开展消融研究（含τ敏感性、轮次预算、消环策略等）。", "bullets": ["【基线模型】：AgentScope, CAMEL, MetaGPT", "【消融实验】：去除query/key机制、固定τ vs 自适应τ、禁用拓扑排序、不同轮次预算设置"]}, "metrics": {"text": "采用任务标准评估协议，关注准确率与效率指标。", "bullets": ["Pass@1", "Exact Match (EM)", "Win Rate", "Avg. Turns", "Token Consumption"]}, "results": {"text": "DyTopo在四大基准上平均提升+6.09%，以更少轮次（2.6轮）达成更高准确率（92.07%），token消耗仅为AgentScope的48%；拓扑演化呈现清晰任务阶段特征，且τ存在任务敏感最优值。", "bullets": ["HumanEval：Pass@1 = 72.3% (+5.8% vs AgentScope)", "APPS：Pass@1 = 41.2% (+7.1% vs AgentScope)", "MATH-500：EM = 53.6% (+6.4% vs AgentScope)", "Omni-MATH：EM = 48.9% (+4.9% vs AgentScope)", "Avg. Turns：2.6 (-3.1 vs AgentScope)", "Token Consumption：48% of AgentScope"]}, "limitations": {"text": "依赖预训练语义编码器质量；对强循环依赖任务的消环策略仍为启发式；自然语言描述符表达粒度受限于LLM生成稳定性。", "bullets": ["【假设强】：query/key文本能充分表征跨Agent语义意图", "【开销大】：all-MiniLM嵌入引入额外延迟（虽免训，但非零开销）", "【泛化边界】：在超长程依赖或强反馈闭环任务中，贪心消环可能牺牲部分语义连贯性"]}}}
{"paper_id": "2602.06043", "title": "Shared LoRA Subspaces for almost Strict Continual Learning", "url": "https://arxiv.org/abs/2602.06043", "year": 2026, "blocks": {"background": {"text": "大模型持续学习面临参数爆炸、灾难性遗忘与数据回放依赖等核心挑战；现有LoRA类方法需为每个任务维护独立适配器，导致参数/内存开销线性增长，难以支持终身学习。", "bullets": ["【痛点】：依赖数据回放或任务标签", "【痛点】：参数量随任务数线性增长", "【现状】：现有LoRA适配器无法跨任务共享，缺乏知识沉淀机制"]}, "objective": {"text": "提出Share方法，在不增加模型参数、不使用数据回放、不维护多个适配器的前提下，实现近严格（near-exact）的参数高效持续学习，并支持跨任务正向与反向知识迁移。", "bullets": ["【任务】：参数高效持续学习", "【核心贡献】：动态构建并演化的共享低秩子空间", "【目标性质】：近严格持续学习（no replay, no extra params, no multi-adapter）"]}, "method": {"text": "将各任务LoRA适配器（A/B矩阵）堆叠并中心化后执行SVD，提取前k个主成分作为冻结共享基向量（α⁰, β⁰），仅训练轻量任务系数εᵗ；持续学习分初始化→持续适配→融合微调三阶段，所有更新均通过解析式操作（SVD、伪逆投影）完成，无需梯度计算。", "bullets": ["【输入】：多任务LoRA适配器集合（Aᵢ, Bᵢ）或新任务数据流/新LoRA", "【架构】：低秩子空间（共享基+任务系数）", "【关键机制】：中心化SVD + Moore-Penrose伪逆系数解析 + 动态基向量融合", "【是否训练】：否（基向量冻结）；是（仅εᵗ为可训练系数，但融合阶段亦可解析求解）", "【创新点】：首次实现梯度无关、数据无关、多任务共享的解析式持续适配"]}, "data": {"text": "实验覆盖自然语言理解、图像分类、3D姿态估计、文本生成四大领域，采用标准基准数据集及社区LoRA资源。", "bullets": ["【数据集】：GLUE (CoLA, SST-2, MNLI, QQP, QNLI, RTE, STS-B, MRPC)", "【数据集】：CIFAR-100", "【数据集】：3DPW (3D pose estimation)", "【数据集】：Flux (text generation)", "【来源】：社区公开LoRA适配器（500+）"]}, "experiment": {"text": "在多任务顺序学习设定下，对比联合训练、iCaRL、AdapterFusion、EigenLoRAx等基线；开展消融研究验证子空间维度k、临时扩展数φ、融合频率的影响；测试混合输入流（新数据+新LoRA）鲁棒性。", "bullets": ["【基线模型】：Joint Training, iCaRL, AdapterFusion, EigenLoRAx, iNeMO", "【消融实验】：k=8/16/32对遗忘率与迁移增益的影响", "【消融实验】：φ=1~5对临时参数开销与融合精度的权衡", "【设置】：顺序任务流 + OOD任务插入 + 社区LoRA即插即用"]}, "metrics": {"text": "采用领域标准评估协议：平均准确率（Avg Acc）、遗忘率（Forgetting）、反向迁移增益（Backward Transfer Δ）、OOD任务Rouge-L、参数量（M）与内存节省倍数。", "bullets": ["Avg Acc", "Forgetting", "Backward Transfer", "Rouge-L", "Pass@1 (implicit in GLUE task-level metrics)"]}, "results": {"text": "Share在多模态持续学习任务中以极低参数量逼近联合训练上界，显著抑制遗忘，并实现可观的正向与反向知识迁移；单组基向量可无损压缩数百LoRA，内存节省达96×。", "bullets": ["GLUE：Avg Acc = 83.44% (+0.46 vs LoRA full, -0.46 vs Joint)", "CoLA：Backward Transfer = +3.81 pts (56.00 → 59.81)", "CIFAR-100：Avg Acc = 94.20%, Forgetting = 0.40%", "3DPW：mAP = 72.3 (surpasses replay-based iNeMO)", "Flux + 500 LoRA：Rouge-L = 55.89 (vs 73.75 for independent LoRA), memory saving = 96×"]}, "limitations": {"text": "方法依赖LoRA结构假设，对非低秩适配形式泛化性待验证；子空间演化需定期SVD，大规模任务数下计算开销渐增；伪逆解析对噪声敏感，未显式建模任务相关性。", "bullets": ["【假设强】：严格依赖LoRA参数化形式（A∈ℝⁿˣʳ, B∈ℝʳˣᵈ）", "【计算开销】：全量SVD融合在任务数>100时成为瓶颈", "【鲁棒性】：伪逆求解对适配器估计噪声敏感，未引入正则化", "【任务建模】：未显式编码任务语义或关系图，跨域迁移理论边界未界定"]}}}
